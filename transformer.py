import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import scipy.io as scio
import numpy as np
import random
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import os
import matplotlib.colors as mcolors
import matplotlib


# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# Transformeræ¨¡å‹ç±»
class TransformerModel(nn.Module):
    def __init__(self, input_dim, embedding_dim, num_heads, num_layers, num_classes, dropout=0.1):
        self.input_dim = input_dim
        super(TransformerModel, self).__init__()

        # è¾“å…¥åµŒå…¥å±‚
        self.embedding = nn.Linear(input_dim, embedding_dim)

        # Positional Encodingï¼šä¸ºæ¯ä¸ªpatchæ·»åŠ ç©ºé—´ä¿¡æ¯
        self.pos_encoding = nn.Parameter(torch.zeros(1, 49, embedding_dim))  # max_len = 49

        # Transformerç¼–ç å™¨å±‚
        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=num_heads,
            dim_feedforward=2048,
            dropout=dropout
        )

        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)

        # åˆ†ç±»å¤´
        self.fc = nn.Linear(embedding_dim, num_classes)

    def forward(self, x):
        # x: (batch, 1, C, H, W)
        x = x.squeeze(1)  # (batch, C, H, W)
        x = x.permute(0, 2, 3, 1)  # (batch, H, W, C)
        x = x.view(x.size(0), -1, self.input_dim)  # (batch, H*W, C)

        x = self.embedding(x)  # Linear projection: (batch, N, embed_dim)
        x = x + self.pos_encoding[:, :x.size(1), :]  # Add positional encoding if needed
        x = self.encoder(x)  # Transformer encoder
        x = x.mean(dim=1)  # Global average pooling
        x = self.fc(x)  # Final classification
        return x


# 3D Patchæå–å‡½æ•°
def extract_3d_patches(img_cube, label_map, patch_size=7, spectral_channels=30, ignored_label=0):
    """
    æå– 3D Patch æ•°æ®ï¼ˆç©ºé—´å¤§å° patch_size Ã— patch_sizeï¼Œå…‰è°±é€šé“æ•°ä¸º spectral_channelsï¼‰ï¼Œ
    ç”¨äº 3D-CNN è®­ç»ƒã€‚è¾“å‡ºæ ¼å¼ (N, 1, C, H, W)ã€‚

    å‚æ•°:
        img_cube: ndarray, (H, W, C) â†’ åŸå§‹é«˜å…‰è°±å›¾åƒæ•°æ®
        label_map: ndarray, (H, W) â†’ æ ‡ç­¾å›¾ï¼ˆ0 ä¸ºèƒŒæ™¯/æ— æ•ˆç±»ï¼‰
        patch_size: int, ç©ºé—´å°ºå¯¸ï¼ˆå»ºè®®å¥‡æ•°ï¼Œé»˜è®¤ 7ï¼‰
        spectral_channels: int, ä½¿ç”¨çš„å…‰è°±é€šé“æ•°ï¼ˆé»˜è®¤ 30ï¼‰
        ignored_label: int, é»˜è®¤ä¸º 0ï¼Œè¡¨ç¤ºå¿½ç•¥èƒŒæ™¯åƒç´ 

    è¿”å›:
        patches: ndarray, shape = (N, 1, spectral_channels, patch_size, patch_size)
        labels:  ndarray, shape = (N,)
    """
    assert patch_size % 2 == 1, "Patch size must be odd."
    H, W, C = img_cube.shape
    pad = patch_size // 2

    # æˆªå–å…‰è°±å‰ spectral_channels ä¸ªæ³¢æ®µ
    img_cube = img_cube[:, :, :spectral_channels]

    # pad spatial ç»´åº¦
    padded_img = np.pad(img_cube, ((pad, pad), (pad, pad), (0, 0)), mode='reflect')
    padded_label = np.pad(label_map, ((pad, pad), (pad, pad)), mode='constant')

    patches = []
    labels = []

    for i in range(pad, H + pad):
        for j in range(pad, W + pad):
            label = padded_label[i, j]
            if label == ignored_label:
                continue
            # æå– patchï¼Œshape: (patch_size, patch_size, C)
            spatial_patch = padded_img[i - pad:i + pad + 1, j - pad:j + pad + 1, :]
            # è½¬æ¢ä¸º (C, H, W)
            patch = np.transpose(spatial_patch, (2, 0, 1))
            # å¢åŠ  batch ç»´åº¦ â†’ (1, C, H, W)
            patch = patch[np.newaxis, ...]
            patches.append(patch)
            labels.append(label - 1)  # ä» 0 å¼€å§‹ç¼–å·

    return np.array(patches, dtype='float32'), np.array(labels, dtype='int64')


# è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
def train_and_evaluate(run_seed=42):
    set_seed(run_seed)

    # -------- æ•°æ®å‡†å¤‡ --------
    h, w = 145, 145
    data_path = 'data/Indian_pines/Indian_pines_corrected.mat'
    label_path = 'data/Indian_pines/Indian_pines_gt.mat'
    data = scio.loadmat(data_path)['indian_pines_corrected'].reshape(-1, 200)
    label = scio.loadmat(label_path)['indian_pines_gt'].flatten()

    data_reshaped = data.reshape(h * w, 200)
    data_cube = data.reshape(h, w, 200)
    label_map = label.reshape(h, w)

    patches, patch_labels = extract_3d_patches(data_cube, label_map, patch_size=7, spectral_channels=30)

    # åˆ’åˆ†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†
    X_train_full, X_test, y_train_full, y_test = train_test_split(
        patches, patch_labels, test_size=0.15, random_state=42, stratify=patch_labels
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full, test_size=0.176, random_state=42, stratify=y_train_full
    )

    # æ—©åœæœºåˆ¶
    best_val_loss = float('inf')
    patience = 10  # å®¹å¿çš„ epoch æ•°
    patience_counter = 0
    best_model_weights = None

    # è½¬ä¸º TensorDataset
    train_loader = DataLoader(
        TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),
        batch_size=128, shuffle=True
    )
    test_loader = DataLoader(
        TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test)),
        batch_size=128, shuffle=False
    )
    val_loader = DataLoader(
        TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val)),
        batch_size=128, shuffle=False
    )

    # æ¨¡å‹å®šä¹‰
    # device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TransformerModel(
        input_dim=30,  # æ¯ä¸ªpatchæœ‰30ä¸ªå…‰è°±é€šé“
        embedding_dim=128,
        num_heads=4,
        num_layers=2,
        num_classes=16,
        dropout=0.3
    ).to(device)

    optimizer = optim.AdamW(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss()

    # æ¨¡å‹è®­ç»ƒ
    train_losses = []
    val_losses = []
    print("å¼€å§‹è®­ç»ƒ Transformer...")
    for epoch in range(1, 201):  # æœ€å¤šè®­ç»ƒ200è½®
        model.train()
        train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # è®¡ç®—éªŒè¯é›† Loss
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                output = model(X_batch)
                loss = criterion(output, y_batch)
                val_loss += loss.item()

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        print(f"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

        # æ—©åœåˆ¤æ–­
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_weights = model.state_dict()  # ä¿å­˜å½“å‰æœ€ä½³æ¨¡å‹
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"â¹ï¸ Early stopping at epoch {epoch}. Best Val Loss: {best_val_loss:.4f}")
                break

    if best_model_weights is not None:
        model.load_state_dict(best_model_weights)

    # æ¨¡å‹æµ‹è¯•
    print("æµ‹è¯•æ¨¡å‹...")
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            output = model(X_batch)
            pred = output.argmax(dim=1)
            correct += (pred == y_batch).sum().item()
            total += y_batch.size(0)

    acc = correct / total * 100
    print(f"âœ… Test Accuracy: {acc:.2f}%")



    # === åˆ†ç±»å›¾åƒå¯è§†åŒ– ===
    matplotlib.rcParams['font.family'] = 'Times New Roman'
    pred_map = np.zeros((h, w), dtype=int)
    mask = (label_map != 0)

    for i in range(h):
        for j in range(w):
            if mask[i, j]:
                patch = data_cube[i - 3:i + 4, j - 3:j + 4, :30]
                if patch.shape != (7, 7, 30):
                    continue
                patch = np.transpose(patch, (2, 0, 1))  # (C, H, W)
                patch = patch[np.newaxis, np.newaxis, ...]  # (1, 1, C, H, W)
                patch_tensor = torch.tensor(patch, dtype=torch.float32).to(device)

                with torch.no_grad():
                    output = model(patch_tensor)
                    pred = output.argmax(dim=1).item()
                pred_map[i, j] = pred + 1  # æ ‡ç­¾ä»1å¼€å§‹

    # å¯è§†åŒ– + ä¿å­˜å›¾åƒ
    colors = plt.colormaps.get_cmap('tab20')
    cmap = mcolors.ListedColormap(colors.colors)

    fig, axs = plt.subplots(1, 2, figsize=(12, 5))
    axs[0].imshow(label_map, cmap=cmap, vmin=1, vmax=16)
    axs[0].set_title("Ground Truth")
    axs[0].axis('off')

    axs[1].imshow(pred_map, cmap=cmap, vmin=1, vmax=16)
    axs[1].set_title(f"Transformer Prediction (Acc: {acc:.2f}%)")
    axs[1].axis('off')

    os.makedirs("figures", exist_ok=True)
    plt.savefig(f"figures/Transformer_run{run_seed}.pdf", bbox_inches='tight')
    plt.close()

    # === æŸå¤±æ›²çº¿å›¾ä¿å­˜ ===
    plt.figure(figsize=(8, 5))
    plt.plot(train_losses, label='Train Loss', linewidth=2)
    plt.plot(val_losses, label='Validation Loss', linewidth=2)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f"Transformer Training Loss (Seed={run_seed}, Acc={acc:.2f}%)")
    plt.legend()
    plt.grid(True)
    plt.savefig(f"figures/Transformer_loss_seed{run_seed}.pdf", bbox_inches='tight')
    plt.close()

    # è¿”å›ç²¾åº¦å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±
    return acc, train_losses, val_losses


# å¤šæ¬¡é‡å¤è®­ç»ƒ
repeats = 10
accuracies = []
train_losses_all = []
val_losses_all = []

for i in range(repeats):
    print(f"\nğŸ” Running trial {i+1} with seed {i*10+42}")
    acc, train_losses, val_losses = train_and_evaluate(run_seed=i*10+42)
    print(f"âœ… Accuracy for run {i+1}: {acc:.2f}%")
    accuracies.append(acc)
    train_losses_all.append(train_losses)
    val_losses_all.append(val_losses)

# è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
mean_acc = np.mean(accuracies)
std_acc = np.std(accuracies)

# è¾“å‡ºä¸ä¿å­˜
print("\nğŸ“Š Summary of Repeated Training:")
for i, acc in enumerate(accuracies):
    print(f"Run {i+1}: {acc:.2f}%")
print(f"\nAverage Accuracy: {mean_acc:.2f}%, Std Dev: {std_acc:.2f}%")

os.makedirs("results", exist_ok=True)
with open("results/transformer_repeat_results.txt", "w") as f:
    for i, acc in enumerate(accuracies):
        f.write(f"Run {i+1}: {acc:.2f}%\n")
    f.write(f"\nAverage Accuracy: {mean_acc:.2f}%\n")
    f.write(f"Standard Deviation: {std_acc:.2f}%\n")

