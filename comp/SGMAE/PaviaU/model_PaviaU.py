# comp/HSIMAE/Botswana/model_Botswana.py
import os
import sys
import time
import json
import random
import numpy as np
import scipy.io as scio
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, cohen_kappa_score
from ptflops import get_model_complexity_info

# ==== ç¡®ä¿èƒ½å¯¼å…¥é™„ä»¶é‡Œçš„ HSIMAE å®ç° ====
# å¦‚æœ Models.py ä¸åœ¨åŒçº§ç›®å½•ï¼Œè¯·æŒ‰éœ€ä¿®æ”¹ä¸‹è¡Œè·¯å¾„ï¼ˆç¤ºä¾‹ï¼šå·¥ç¨‹æ ¹ç›®å½•ï¼‰
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../")))
try:
    from Models import HSIViT   # æ¥è‡ªä½ æä¾›çš„ HSIMAE æºç ï¼ˆé™„ä»¶ Models.pyï¼‰
except Exception as e:
    raise ImportError(
        f"æ— æ³•å¯¼å…¥ Models.HSIViTï¼š{e}\n"
        f"è¯·ç¡®è®¤ Models.py åœ¨ PYTHONPATH æˆ–ä¸æœ¬è„šæœ¬åŒçº§ç›®å½•ã€‚"
    )


# ===================== é€šç”¨å·¥å…· / ä¸2Dè„šæœ¬ä¿æŒä¸€è‡´ =====================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def extract_2d_patches(img_cube, label_map, patch_size=7, ignored_label=0):
    """
    è¾“å…¥ (H, W, C)  â†’ è¾“å‡º (N, C, patch, patch)ï¼Œä»…ä¿ç•™ä¸­å¿ƒåƒç´ æœ‰æ ‡ç­¾çš„ patchï¼Œæ ‡ç­¾ä»0å¼€å§‹
    """
    assert patch_size % 2 == 1, "patch_size must be odd."
    H, W, C = img_cube.shape
    pad = patch_size // 2
    if label_map.shape != (H, W):
        raise ValueError(f"label_map shape {label_map.shape} != image spatial shape {(H, W)}")
    if C < 1:
        raise ValueError("img_cube must have at least 1 channel")
    if np.all(label_map == ignored_label):
        raise ValueError("All labels equal to ignored_label; no samples to extract.")

    padded_img = np.pad(img_cube, ((pad, pad), (pad, pad), (0, 0)), mode='reflect')
    padded_label = np.pad(label_map, ((pad, pad), (pad, pad)), mode='constant', constant_values=ignored_label)

    patches, labels = [], []
    for i in range(pad, H + pad):
        for j in range(pad, W + pad):
            lab = padded_label[i, j]
            if lab == ignored_label:
                continue
            patch = padded_img[i - pad:i + pad + 1, j - pad:j + pad + 1, :]
            patch = np.transpose(patch, (2, 0, 1))  # (C,H,W)
            patches.append(patch)
            labels.append(int(lab - 1))
    return np.asarray(patches, dtype='float32'), np.asarray(labels, dtype='int64')


def evaluate_and_log_metrics(y_true, y_pred, model, run_seed, dataset_name, acc,
                             in_channels, patch_size, log_root, train_time=None, val_time=None):
    os.makedirs(log_root, exist_ok=True)

    conf_mat = confusion_matrix(y_true, y_pred)
    with np.errstate(divide='ignore', invalid='ignore'):
        per_class_acc = conf_mat.diagonal() / conf_mat.sum(axis=1)
        per_class_acc = np.nan_to_num(per_class_acc, nan=0.0)
    aa = float(per_class_acc.mean())
    kappa = float(cohen_kappa_score(y_true, y_pred))

    with open(os.path.join(log_root, "metrics.json"), "w") as f:
        json.dump({
            "seed": run_seed,
            "dataset": dataset_name,
            "overall_accuracy": round(float(acc), 4),
            "average_accuracy": round(aa, 4),
            "kappa": round(kappa, 4),
            "per_class_accuracy": [round(float(x), 4) for x in per_class_acc.tolist()]
        }, f, indent=2)

    # FLOPs/Params â€”â€” ä»¥ (in_channels, P, P) ä½œä¸º dummy è¾“å…¥
    try:
        flops, params = get_model_complexity_info(
            model, (in_channels, patch_size, patch_size),
            as_strings=False, print_per_layer_stat=False
        )
        flops_info = {"FLOPs(M)": round(flops / 1e6, 2), "Params(K)": round(params / 1e3, 2)}
    except Exception:
        total_params = sum(p.numel() for p in model.parameters())
        flops_info = {"FLOPs(M)": None, "Params(K)": round(total_params / 1e3, 2)}

    with open(os.path.join(log_root, "model_profile.json"), "w") as f:
        json.dump(flops_info, f, indent=2)

    with open(os.path.join(log_root, "time_log.json"), "w") as f:
        json.dump({
            "train_time(s)": round(train_time, 2) if train_time else None,
            "val_time(s)": round(val_time, 2) if val_time else None
        }, f, indent=2)

    np.savetxt(os.path.join(log_root, "confusion_matrix.csv"), conf_mat, fmt="%d", delimiter=",")


# ===================== HSIMAE åˆ†ç±»åŒ…è£…å™¨ =====================
class HSIMAEClassifier(nn.Module):
    """
    å°† HSIMAE çš„ HSIViT åˆ†ç±»åˆ†æ”¯å°è£…ä¸ºä¸ä½ å½“å‰è®­ç»ƒè„šæœ¬å…¼å®¹çš„æ¨¡å‹ï¼š
    å‰å‘è¾“å…¥: (B, C, P, P) â€”â€” å…¶ä¸­ C æ˜¯ PCA åçš„é€šé“æ•°ï¼›P æ˜¯ patch_size
    å†…éƒ¨æŠŠ C å½“ä½œ HSIMAE çš„ bandsï¼ˆå…‰è°±é•¿åº¦ï¼‰ï¼Œå¹¶ reshape æˆ (B, 1, T=C, P, P)
    """
    def __init__(
        self,
        in_channels: int,      # = PCAåçš„é€šé“ (bands)
        num_classes: int,
        patch_size: int = 7,
        embed_dim: int = 128,  # è½»é‡é…ç½®ï¼Œå¯æŒ‰éœ€è°ƒå¤§
        depth: int = 6,
        s_depth: int = 0,      # 0 è¡¨ç¤ºä¸åš dual-branch çš„å°å—å¾ªç¯ï¼Œç®€å•ä¸€äº›
        num_heads: int = 8,
        mlp_ratio: float = 4.0,
        drop_path: float = 0.0,
    ):
        super().__init__()
        # å…³é”®è®¾å®šï¼š
        #   img_size = patch_sizeï¼Œpatch_size åŒä¸º patch_size â€”â€” è¿™æ ·ç©ºé—´ä¸Šåªæœ‰ 1Ã—1 ä¸ª token
        #   bands = in_channelsï¼Œb_patch_size = in_channels â€”â€” å…‰è°±æ–¹å‘åªåˆ‡ 1 ä¸ª token
        # è¿™ä½¿å¾— HSIViT åˆšå¥½å¯¹ä¸€ä¸ª patch åšâ€œé€šé“æ³¨æ„åŠ› + è½»é‡Transformerâ€åˆ†ç±»ï¼Œä¸”ä¸æ•°æ®ç®¡çº¿å®Œç¾å…¼å®¹ã€‚
        self.core = HSIViT(
            img_size=patch_size,
            patch_size=patch_size,
            in_chans=1,               # æˆ‘ä»¬æŠŠ (C,P,P) ä½œä¸º (T=bands,H,W)ï¼Œå› æ­¤è¾“å…¥é€šé“æ˜¯ 1
            embed_dim=embed_dim,
            depth=depth,
            s_depth=s_depth,
            num_heads=num_heads,
            mlp_ratio=mlp_ratio,
            norm_layer=nn.LayerNorm,
            bands=in_channels,        # å…‰è°±é•¿åº¦ = PCAé€šé“æ•°
            b_patch_size=in_channels, # ä¸€æ¬¡æ€§åƒæ‰å…¨éƒ¨ bandsï¼ˆç¡®ä¿ T % u == 0ï¼‰
            num_class=num_classes,
            no_qkv_bias=False,
            trunc_init=False,
            drop_path=drop_path,
        )

    def forward(self, x):
        # x: (B, C, P, P) -> (B, 1, T=C, P, P)
        x = x.unsqueeze(1)
        # HSIViT.forward è¿”å› (B, num_classes)
        return self.core(x)


# ===================== è®­ç»ƒä¸è¯„ä¼°ï¼ˆä¸2Dè„šæœ¬ä¸€è‡´ï¼‰ =====================
def train_and_evaluate_hsimae(run_seed=42,
                              dataset_name="Botswana",
                              data_path='../../root/data/HSI_dataset/Pavia_university/PaviaU.mat',
                              label_path='/root/data/HSI_dataset/Pavia_university/PaviaU_gt.mat',
                              pca_components=30,
                              patch_size=7,
                              batch_size=128,
                              max_epochs=200,
                              patience=10,
                              lr=1e-3,
                              embed_dim=128,
                              depth=6,
                              s_depth=0,
                              num_heads=8,
                              mlp_ratio=4.0,
                              drop_path=0.0):
    set_seed(run_seed)

    # ---- æ•°æ®åŠ è½½ ----
    img = scio.loadmat(data_path)['paviaU']
    label = scio.loadmat(label_path)['paviaU_gt'].flatten()
    h, w, bands = img.shape
    data_reshaped = img.reshape(h * w, bands)
    label_map = label.reshape(h, w)

    # ---- PCA é™ç»´åˆ° in_channels=C ----
    C = min(pca_components, bands)
    img_2d = img.reshape(-1, bands)
    pca = PCA(n_components=C)
    img_pca = pca.fit_transform(img_2d).reshape(h, w, C)

    # ---- ç”Ÿæˆ 2D Patch ----
    patches, patch_labels = extract_2d_patches(img_pca, label_map, patch_size=patch_size, ignored_label=0)
    num_classes = int(patch_labels.max()) + 1
    in_channels = C

    # ---- åˆ’åˆ† 70/15/15 ----
    X_train_full, X_test, y_train_full, y_test = train_test_split(
        patches, patch_labels, test_size=0.15, stratify=patch_labels, random_state=42
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full, test_size=0.176, stratify=y_train_full, random_state=42
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),
                              batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val)),
                            batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test)),
                             batch_size=batch_size, shuffle=False)

    # ---- æ¨¡å‹/ä¼˜åŒ–å™¨/æŸå¤± ----
    model = HSIMAEClassifier(
        in_channels=in_channels,
        num_classes=num_classes,
        patch_size=patch_size,
        embed_dim=embed_dim,
        depth=depth,
        s_depth=s_depth,
        num_heads=num_heads,
        mlp_ratio=mlp_ratio,
        drop_path=drop_path,
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()

    best_val_loss = float('inf'); patience_counter = 0
    best_model_weights = None
    train_losses, val_losses = [], []
    train_start = time.time()

    # ---- è®­ç»ƒ ----
    for epoch in range(1, max_epochs + 1):
        model.train(); train_loss = 0.0
        for Xb, yb in train_loader:
            Xb, yb = Xb.to(device), yb.to(device)
            optimizer.zero_grad()
            out = model(Xb)                 # (B, num_classes)
            loss = criterion(out, yb)
            loss.backward(); optimizer.step()
            train_loss += loss.item()

        model.eval(); val_loss = 0.0
        with torch.no_grad():
            for Xb, yb in val_loader:
                Xb, yb = Xb.to(device), yb.to(device)
                val_loss += criterion(model(Xb), yb).item()

        print(f"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        train_losses.append(train_loss); val_losses.append(val_loss)
        if val_loss < best_val_loss:
            best_val_loss = val_loss; best_model_weights = model.state_dict(); patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"â¹ï¸ Early stopping at epoch {epoch}")
                break

    train_time = time.time() - train_start

    # ---- æµ‹è¯• ----
    if best_model_weights is not None:
        model.load_state_dict(best_model_weights)
    model.eval(); all_true, all_pred = [], []
    val_start = time.time()
    with torch.no_grad():
        for Xb, yb in test_loader:
            Xb = Xb.to(device)
            preds = model(Xb).argmax(dim=1)
            all_true.extend(yb.numpy()); all_pred.extend(preds.cpu().numpy())
    acc = np.mean(np.array(all_true) == np.array(all_pred)) * 100.0
    val_time = time.time() - val_start
    print(f"âœ… Test Accuracy: {acc:.2f}%")

    # ---- æ—¥å¿— ----
    log_root = f"comp_logs/HSIMAE/{dataset_name}/seed_{run_seed}"
    os.makedirs(log_root, exist_ok=True)
    with open(os.path.join(log_root, "loss_curve.json"), "w") as f:
        json.dump({
            "train_loss": [round(float(l), 4) for l in train_losses],
            "val_loss": [round(float(l), 4) for l in val_losses]
        }, f, indent=2)

    evaluate_and_log_metrics(
        y_true=np.array(all_true),
        y_pred=np.array(all_pred),
        model=model,
        run_seed=run_seed,
        dataset_name=str(dataset_name),
        acc=acc,
        in_channels=in_channels,
        patch_size=patch_size,
        train_time=train_time,
        val_time=val_time,
        log_root=log_root,
    )

    return acc


if __name__ == "__main__":
    # ====== æ•°æ®é›†é…ç½®ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ ======
    dataset_name = "PaviaU"

    pca_components = 30
    patch_size = 7
    repeats = 1

    accs = []
    for i in range(repeats):
        seed = i * 10 + 42
        print(f"\nğŸ” Running trial {i+1} with seed {seed}")
        acc = train_and_evaluate_hsimae(
            run_seed=seed,
            dataset_name=dataset_name,
            pca_components=pca_components,
            patch_size=patch_size,
            batch_size=128,
            max_epochs=200,
            patience=10,
            lr=1e-3,
            embed_dim=128,    # å¯è°ƒï¼ˆå˜å¤§æ›´å¼ºã€ä¹Ÿæ›´æ…¢ï¼‰
            depth=6,
            s_depth=0,        # å¦‚æƒ³å¯ç”¨ dual-branch çš„å°å—å †å ï¼Œå¯è®¾ >0
            num_heads=8,
            mlp_ratio=4.0,
            drop_path=0.0,
        )
        accs.append(acc)

    print("\nğŸ“Š Summary of Repeated Training:")
    for i, acc in enumerate(accs):
        print(f"Run {i+1}: {acc:.2f}%")
    print(f"\nAverage Accuracy: {np.mean(accs):.2f}%, Std Dev: {np.std(accs):.2f}%")